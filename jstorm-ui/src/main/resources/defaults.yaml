########### These all have default values as shown
########### Additional configuration goes into storm.yaml

java.library.path: "/usr/local/lib:/opt/local/lib:/usr/lib"

### storm.* configs are general configurations
# the local dir is where jars are kept
storm.local.dir: "jstorm-local"
storm.zookeeper.servers:
    - "localhost"
storm.zookeeper.port: 2181
storm.zookeeper.root: "/jstorm"
storm.zookeeper.session.timeout: 20000
storm.zookeeper.connection.timeout: 15000
storm.zookeeper.retry.times: 5
storm.zookeeper.retry.interval: 1000
storm.zookeeper.retry.intervalceiling.millis: 30000
storm.cluster.mode: "distributed" # can be distributed or local
storm.local.mode.zmq: false
storm.thrift.transport: "backtype.storm.security.auth.SimpleTransportPlugin"
#storm.messaging.transport: "com.alibaba.jstorm.message.zeroMq.MQContext"
storm.messaging.transport: "com.alibaba.jstorm.message.netty.NettyContext"

### nimbus.* configs are for the master
nimbus.host: "localhost"
nimbus.thrift.port: 7627
nimbus.thrift.max_buffer_size: 1048576
nimbus.childopts: " -Xms1g -Xmx1g -Xmn256m -XX:PermSize=96m -XX:+UseConcMarkSweepGC  -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:CMSFullGCsBeforeCompaction=5 -XX:+HeapDumpOnOutOfMemoryError  -XX:+UseCMSCompactAtFullCollection -XX:CMSMaxAbortablePrecleanTime=5000 "
nimbus.task.timeout.secs: 60
nimbus.supervisor.timeout.secs: 60
nimbus.monitor.freq.secs: 10
nimbus.cleanup.inbox.freq.secs: 600
nimbus.inbox.jar.expiration.secs: 3600
nimbus.task.launch.secs: 240
nimbus.reassign: true
nimbus.file.copy.expiration.secs: 600
nimbus.topology.validator: "backtype.storm.nimbus.DefaultTopologyValidator"
nimbus.classpath: ""

### ui.* configs are for the master
ui.port: 8080
ui.childopts: " -Xms1g -Xmx1g -Xmn256m -XX:PermSize=96m -XX:+UseConcMarkSweepGC  -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:CMSFullGCsBeforeCompaction=5 -XX:+HeapDumpOnOutOfMemoryError  -XX:+UseCMSCompactAtFullCollection -XX:CMSMaxAbortablePrecleanTime=5000 "

drpc.port: 4772
drpc.worker.threads: 64
drpc.queue.size: 128
drpc.invocations.port: 4773
drpc.request.timeout.secs: 600
drpc.childopts: " -Xms1g -Xmx1g -Xmn256m -XX:PermSize=96m -Xmn128m -XX:PermSize=64m -XX:+UseConcMarkSweepGC  -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:CMSFullGCsBeforeCompaction=5 -XX:+HeapDumpOnOutOfMemoryError  -XX:+UseCMSCompactAtFullCollection -XX:CMSMaxAbortablePrecleanTime=5000 "

transactional.zookeeper.root: "/transactional"
transactional.zookeeper.servers: null
transactional.zookeeper.port: null

### supervisor.* configs are for node supervisors
# Define the amount of workers that can be run on this machine. Each worker is assigned a port to use for communication
supervisor.slots.ports:
    - 6800
    - 6801
    - 6802
    - 6803
supervisor.childopts: " -Xms512m -Xmx512m -Xmn128m -XX:PermSize=64m -XX:+UseConcMarkSweepGC  -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:CMSFullGCsBeforeCompaction=5 -XX:+HeapDumpOnOutOfMemoryError  -XX:+UseCMSCompactAtFullCollection -XX:CMSMaxAbortablePrecleanTime=5000 "
#how long supervisor will wait to ensure that a worker process is started
supervisor.worker.start.timeout.secs: 120
#how long between heartbeats until supervisor considers that worker dead and tries to restart it
supervisor.worker.timeout.secs: 60
#how frequently the supervisor checks on the status of the processes it's monitoring and restarts if necessary
supervisor.monitor.frequency.secs: 10
#how frequently the supervisor heartbeats to the cluster state (for nimbus)
supervisor.heartbeat.frequency.secs: 60
supervisor.enable: true
#if set null, it will be get by system
supervisor.hostname: null


#How many cpu slot one supervisor can support , if it is null, it will be set by JStorm
supervisor.cpu.slot.num: null

#How many memory slot one supervisor can support , if it is null, it will be set by JStorm
supervisor.mem.slot.num: null

# How much disk slot one supervisor can support
# if it is null, it will use $(storm.local.dir)/worker_shared_data
supervisor.disk.slot: null
# if use multiple disks, it can be set as the following
#supervisor.disk.slot:
#	- /disk0/jstorm/data
#	- /disk1/jstorm/data
#	- /disk2/jstorm/data
#	- /disk3/jstorm/data



### worker.* configs are for task workers
# worker gc configuration
# worker.gc.path will put all gc logs and memory dump file
worker.gc.path: "%JSTORM_HOME%/logs"
worker.gc.childopts: " -XX:SurvivorRatio=4 -XX:+UseConcMarkSweepGC  -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:CMSFullGCsBeforeCompaction=5 -XX:+HeapDumpOnOutOfMemoryError -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseCMSCompactAtFullCollection -XX:CMSMaxAbortablePrecleanTime=5000 "
worker.heartbeat.frequency.secs: 2
worker.classpath: ""
worker.redirect.output: true

task.heartbeat.frequency.secs: 10
task.refresh.poll.secs: 10

zmq.threads: 1
zmq.linger.millis: 5000
zmq.hwm: 0

storm.messaging.netty.server_worker_threads: 1
storm.messaging.netty.client_worker_threads: 1
storm.messaging.netty.buffer_size: 5242880 #5MB buffer
storm.messaging.netty.max_retries: 30
storm.messaging.netty.max_wait_ms: 1000
storm.messaging.netty.min_wait_ms: 100
storm.messaging.netty.disruptor: true

### topology.* configs are for specific executing storms
topology.enable.message.timeouts: true
topology.debug: false
topology.optimize: true
topology.workers: 1
topology.acker.executors: null
topology.tasks: null
# maximum amount of time a message has to complete before it's considered failed
topology.message.timeout.secs: 30
topology.skip.missing.kryo.registrations: false
topology.max.task.parallelism: null
topology.max.spout.pending: null
topology.state.synchronization.timeout.secs: 60
topology.stats.sample.rate: 0.05
topology.builtin.metrics.bucket.size.secs: 60
topology.fall.back.on.java.serialization: true
topology.worker.childopts: null
topology.executor.receive.buffer.size: 1024 #batched
topology.executor.send.buffer.size: 1024 #individual messages
topology.receiver.buffer.size: 8 # setting it too high causes a lot of problems (heartbeat thread gets starved, throughput plummets)
topology.transfer.buffer.size: 1024 # batched
topology.tick.tuple.freq.secs: null
topology.worker.shared.thread.pool.size: 4
topology.disruptor.wait.strategy: "com.lmax.disruptor.BlockingWaitStrategy"
topology.spout.wait.strategy: "backtype.storm.spout.SleepSpoutWaitStrategy"
topology.sleep.spout.wait.strategy.time.ms: 1
topology.error.throttle.interval.secs: 10
topology.max.error.report.per.interval: 5
topology.kryo.factory: "backtype.storm.serialization.DefaultKryoFactory"
topology.tuple.serializer: "backtype.storm.serialization.types.ListDelegateSerializer"
topology.trident.batch.emit.interval.millis: 500

# set topology resource assign priority weight
# the new topology resource assign resource will according to CPU/memory/disk/port 4 part
# so different weight, different priority
topology.disk.weight: 5
topology.cpu.weight: 3
topology.memory.weight: 1
topology.port.weight: 2

# if this is false, select supervisor whose weight is highest(left_disk_slot_num * disk.weight 
# + left_cpu_slot_num * cpu.weight + left_memory_slot_num * memory.weight + 
# left_port_slot_num * port.weight)
# it this is true, select which supervisor is best one according to slot level by level, 
# select the supervisor whose disk slot is the most
# if the last select result isn't 1, then select supervisor according to cpu
# then according to port, last  according to memory
topology.assign.supervisor.bylevel: true

#the following weight will decide which task will be assign firstly
topology.task.on.different.node.weight: 2
topology.use.old.assign.ratio.weight: 2
topology.user.define.assign.ratio.weight: 2


# enable topology use user-define classloader to avoid class conflict
topology.enable.classloader: false

# enable supervisor use cgroup to make resource isolation
# Before enable it, you should make sure:
# 	1. Linux version (>= 2.6.18)
# 	2. Have installed cgroup (check the file's existence:/proc/cgroups)
#	3. You should start your supervisor on root
# You can get more about cgroup:
#   http://t.cn/8s7nexU
supervisor.enable.cgroup: false

dev.zookeeper.path: "/tmp/dev-storm-zookeeper"

#if this configuration has been set, 
# the spout or bolt will log all received tuples
# topology.debug just for logging all sent tuples 
topology.debug.recv.tuple: false

#Usually, spout finish preparation before bolt, 
#so spout need wait several seconds so that bolt finish preparation
# the default setting is 30 seconds
spout.delay.run: 30

# One memory slot size, the unit size of memory, the default setting is 1G
# For example , if it is 1G, and one task set "memory.slots.per.task" as 2
# then the task will use 2G memory
memory.slot.per.size: 1073741824

# This weight means the number of logic cpu slots corresponding to per hardware cpu core
# For example , if a supervisor have 4 cpu cores and set this weight as 2 , this supervisor
# will have 8 logic cpu slots. This weight also effects the cgroup , if this weight is 1 ,
# one cpu slot corresponds to 1024 of cgroup's cpu weight ; when you set this weight as 2 , 
# one cpu slot corresponds to 512 of cgroup's cpu weight ;
# You can get more about cgroup cpu weight(cpu.share):
#     http://t.cn/8std8UV 
cpu.slot.per.weight: 2

# if you want to start nimbus with group model, you should 
# set your group file path like follow:
# use absolute path
# nimbus.groupfile.path: /home/admin/jstorm/conf/group_file.ini
nimbus.groupfile.path: null


# if you want to submit topology on the nimbus with group
# module, you must set your gourp name (user.group)here or 
# in your topology;The user.name and the user.password is
# useless in this version 
user.group: null
user.name: null
user.password: null


# container setting means jstorm is running under other system, such as hadoop-yarn/Ali-Apsara
# For example, in Ali-Apsara, Fuxi start c++ container process, 
# the C++ container fork process to start nimbus or supervisor 
container.heartbeat.timeout.seconds: 240
container.heartbeat.frequence: 10


# enable java sandbox or not
java.sandbox.enable: false.
