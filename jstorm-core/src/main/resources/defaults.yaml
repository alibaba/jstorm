# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


########### These all have default values as shown
########### Additional configuration goes into storm.yaml

java.library.path: "/usr/local/lib:/opt/local/lib:/usr/lib:/home/admin/jstorm/lib"

### storm.* configs are general configurations
# the local dir is where jars are kept
storm.local.dir: "jstorm-local"
storm.zookeeper.servers:
    - "localhost"
storm.zookeeper.port: 2181
storm.zookeeper.root: "/jstorm"
storm.zookeeper.session.timeout: 20000
storm.zookeeper.connection.timeout: 15000
storm.zookeeper.retry.times: 20
storm.zookeeper.retry.interval: 1000
storm.zookeeper.retry.intervalceiling.millis: 30000
storm.zookeeper.auth.user: null
storm.zookeeper.auth.password: null
storm.exhibitor.port: 8080
storm.exhibitor.poll.uripath: "/exhibitor/v1/cluster/list"
storm.cluster.mode: "distributed" # can be distributed or local
storm.local.mode.zmq: false
storm.thrift.transport: "backtype.storm.security.auth.SimpleTransportPlugin"
storm.principal.tolocal: "backtype.storm.security.auth.DefaultPrincipalToLocal"
storm.group.mapping.service: "backtype.storm.security.auth.ShellBasedGroupsMapping"
storm.messaging.transport: "com.alibaba.jstorm.message.netty.NettyContext"
storm.nimbus.retry.times: 5
storm.nimbus.retry.interval.millis: 2000
storm.nimbus.retry.intervalceiling.millis: 60000
storm.auth.simple-white-list.users: []
storm.auth.simple-acl.users: []
storm.auth.simple-acl.users.commands: []
storm.auth.simple-acl.admins: []
storm.cluster.state.store: "backtype.storm.cluster.ZKStateStorageFactory"
storm.meta.serialization.delegate: "backtype.storm.serialization.DefaultSerializationDelegate"
storm.codedistributor.class: "backtype.storm.codedistributor.LocalFileSystemCodeDistributor"
storm.workers.artifacts.dir: "workers-artifacts"
storm.health.check.dir: "healthchecks"
storm.health.check.timeout.ms: 5000


### nimbus.* configs are for the master
nimbus.host: "localhost"
nimbus.thrift.port: 8627
nimbus.thrift.threads: 64
nimbus.thrift.max_buffer_size: 16384000
nimbus.childopts: " -Xms4g -Xmx4g -Xmn1536m -XX:PermSize=256m  -XX:SurvivorRatio=4 -XX:+UseConcMarkSweepGC  -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:CMSFullGCsBeforeCompaction=5 -XX:+HeapDumpOnOutOfMemoryError -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=100M -XX:+UseCMSCompactAtFullCollection -XX:CMSMaxAbortablePrecleanTime=5000 "
nimbus.task.timeout.secs: 120
nimbus.supervisor.timeout.secs: 180
nimbus.monitor.freq.secs: 10
nimbus.cleanup.inbox.freq.secs: 600
nimbus.inbox.jar.expiration.secs: 3600
nimbus.code.sync.freq.secs: 120
nimbus.task.launch.secs: 240
nimbus.reassign: true
nimbus.file.copy.expiration.secs: 120
nimbus.topology.validator: "backtype.storm.nimbus.DefaultTopologyValidator"
topology.min.replication.count: 1
topology.max.replication.wait.time.sec: 60
nimbus.classpath: ""
## which external plugin to use, e.g. "hbase,hdfs", jstorm will automate load jstorm-hbase and jstorm-hdfs classpath
nimbus.external: ""
nimbus.use.ip: true
nimbus.credential.renewers.freq.secs: 600
nimbus.impersonation.authorizer: "backtype.storm.security.auth.authorizer.ImpersonationAuthorizer"
## Two type cache 
## "com.alibaba.jstorm.cache.TimeoutMemCache" is suitable for small cluster
## "com.alibaba.jstorm.cache.TimeoutMemCache" can only run under linux/mac, it is suitable for huge cluster
## if it is null, it will detected by environment
nimbus.cache.class: null
## if this is true, nimbus db cache will be reset when start nimbus
nimbus.cache.reset: true
cache.timeout.list: null
nimbus.impersonation.authorizer: "backtype.storm.security.auth.authorizer.ImpersonationAuthorizer"
nimbus.queue.size: 100000
scheduler.display.resource: false

cluster.name: "default"
### ui.* configs are for the master
ui.port: 8080
ui.childopts: " -Xms1g -Xmx1g -Xmn256m -XX:PermSize=96m -XX:+UseConcMarkSweepGC  -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:CMSFullGCsBeforeCompaction=5 -XX:+HeapDumpOnOutOfMemoryError  -XX:+UseCMSCompactAtFullCollection -XX:CMSMaxAbortablePrecleanTime=5000 "
ui.actions.enabled: true
ui.filter: null
ui.filter.params: null
ui.users: null
ui.header.buffer.bytes: 4096
ui.http.creds.plugin: backtype.storm.security.auth.DefaultHttpCredentialsPlugin

logviewer.port: 8000
logviewer.childopts: "-Xmx128m"
logviewer.cleanup.age.mins: 10080
logviewer.appender.name: "A1"
logviewer.max.sum.worker.logs.size.mb: 4096
logviewer.max.per.worker.logs.size.mb: 2048

logs.users: null
drpc.port: 4772
drpc.worker.threads: 64
drpc.max_buffer_size: 31457280
drpc.queue.size: 128
drpc.invocations.port: 4773
drpc.invocations.threads: 64
drpc.request.timeout.secs: 600
drpc.childopts: " -Xms1g -Xmx1g -Xmn256m -XX:PermSize=96m -Xmn128m -XX:PermSize=64m -XX:+UseConcMarkSweepGC  -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:CMSFullGCsBeforeCompaction=5 -XX:+HeapDumpOnOutOfMemoryError -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=100M -XX:+UseCMSCompactAtFullCollection -XX:CMSMaxAbortablePrecleanTime=5000 "
drpc.http.port: 3774
drpc.https.port: -1
drpc.https.keystore.password: ""
drpc.https.keystore.type: "JKS"
drpc.http.creds.plugin: backtype.storm.security.auth.DefaultHttpCredentialsPlugin
drpc.authorizer.acl.filename: "drpc-auth-acl.yaml"
drpc.authorizer.acl.strict: false

transactional.zookeeper.root: "/transactional"
transactional.zookeeper.servers: null
transactional.zookeeper.port: null


# 
# 
# ##### These may optionally be filled in:
#    
## Map of tokens to a serialization class. tokens less than 32 are reserved by storm.
## Tokens are written on the wire to identify the field.
# topology.serializations: 
#     - "org.mycompany.MyObjectSerialization"
#     - "org.mycompany.MyOtherObjectSerialization"
## Locations of the drpc servers
drpc.servers:
     - "localhost"

### supervisor.* configs are for node supervisors
# Define the amount of workers that can be run on this machine. Each worker is assigned a port to use for communication

# if supervisor.slots.ports is null, 
# the port list will be generated by cpu cores and system memory size 
# for example, 
# there are cpu_num = system_physical_cpu_num/supervisor.slots.port.cpu.weight
# there are mem_num = system_physical_memory_size/(worker.memory.size * supervisor.slots.port.mem.weight) 
# The final port number is min(cpu_num, mem_num)
supervisor.slots.ports.base: 6900
supervisor.slots.port.cpu.weight: 1.2
supervisor.slots.port.mem.weight: 0.8
supervisor.slots.ports: null
#supervisor.slots.ports:
#    - 6800
#    - 6801
#    - 6802
#    - 6803
supervisor.childopts: " -Xms512m -Xmx512m -Xmn256m -XX:PermSize=64m -XX:+UseConcMarkSweepGC  -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:CMSFullGCsBeforeCompaction=5 -XX:+HeapDumpOnOutOfMemoryError -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=100M -XX:+UseCMSCompactAtFullCollection -XX:CMSMaxAbortablePrecleanTime=5000 "
supervisor.run.worker.as.user: false
#how long supervisor will wait to ensure that a worker process is started
supervisor.worker.start.timeout.secs: 120
#how long between heartbeats until supervisor considers that worker dead and tries to restart it
supervisor.worker.timeout.secs: 240
#how many seconds to sleep for before shutting down threads on worker
supervisor.worker.shutdown.sleep.secs: 1
#how frequently the supervisor checks on the status of the processes it's monitoring and restarts if necessary
supervisor.monitor.frequency.secs: 10
#how frequently the supervisor heartbeats to the cluster state (for nimbus)
supervisor.heartbeat.frequency.secs: 60
supervisor.enable: true
#if set null, it will be get by system
supervisor.hostname: null
# use ip
supervisor.use.ip: true
supervisor.supervisors: []
supervisor.supervisors.commands: []
supervisor.memory.capacity.mb: 4096.0
#By convention 1 cpu core should be about 100, but this can be adjusted if needed
# using 100 makes it simple to set the desired value to the capacity measurement
# for single threaded bolts
supervisor.cpu.capacity: 400.0
supervisor.external: ""

### worker.* configs are for task workers
# worker gc configuration
# worker.gc.path will put all gc logs and memory dump file
worker.gc.childopts: " -XX:SurvivorRatio=4 -XX:+UseConcMarkSweepGC  -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:CMSFullGCsBeforeCompaction=5 -XX:+HeapDumpOnOutOfMemoryError -XX:+UseCMSCompactAtFullCollection -XX:CMSMaxAbortablePrecleanTime=5000 "
# Unlocking commercial features requires a special license from Oracle.
# See http://www.oracle.com/technetwork/java/javase/terms/products/index.html
# For this reason, profiler features are disabled by default.
worker.profiler.enabled: false
worker.profiler.childopts: "-XX:+UnlockCommercialFeatures -XX:+FlightRecorder"
worker.profiler.command: "flight.bash"

# check whether dynamic log levels can be reset from DEBUG to INFO in workers
worker.log.level.reset.poll.secs: 30
worker.heartbeat.frequency.secs: 2
worker.metric.report.frequency.secs: 60
worker.classpath: ""
worker.redirect.output: false
# if worker.redirect.output.file is null, then it will be $LOG.out
# please use absolute path
worker.redirect.output.file: null
worker.external: ""

# control how many worker receiver threads we need per worker
topology.worker.receiver.thread.count: 1

# when supervisor is shutdown, automatically shutdown worker
worker.stop.without.supervisor: false
worker.memory.size: 2147483648
# you can set the worker's memory which contain tm, and the default value is the same as worker.memory.size
topology.master.worker.memory.size: null
# when min memory size is null, set Xmx as Xms
worker.memory.min.size: null

task.heartbeat.frequency.secs: 10
task.refresh.poll.secs: 10
# how long task do cleanup 
task.cleanup.timeout.sec: 10
task.credentials.poll.secs: 30

zmq.threads: 1
zmq.linger.millis: 5000
zmq.hwm: 0

# Netty thread num: 0 means no limit
storm.messaging.netty.server_worker_threads: 5
storm.messaging.netty.client_worker_threads: 2
storm.messaging.netty.buffer_size: 5242880 #5MB buffer
storm.messaging.netty.receive.buffer_size: 20971520 #20MB
storm.messaging.netty.max_retries: 30
storm.messaging.netty.max_wait_ms: 1000
storm.messaging.netty.min_wait_ms: 100
storm.messaging.netty.disruptor: true
# If async and batch is used in netty transfer, netty will batch message
storm.messaging.netty.transfer.async.batch: true
# If the Netty messaging layer is busy(netty internal buffer not writable), the Netty client will try to batch message as more as possible up to the size of storm.messaging.netty.transfer.batch.size bytes, otherwise it will try to flush message as soon as possible to reduce latency.
storm.messaging.netty.transfer.batch.size: 65536
# Sets the backlog value to specify when the channel binds to a local address
storm.messaging.netty.socket.backlog: 500
# We check with this interval that whether the Netty channel is writable and try to write pending messages if it is.
storm.messaging.netty.flush.check.interval.ms: 5
# when netty connection is broken, 
# when buffer size is more than storm.messaging.netty.buffer.threshold
# it will slow down the netty sending speed
storm.messaging.netty.buffer.threshold: 8388608
storm.messaging.netty.max.pending: 4
## when netty is in sync mode and client channel is unavailable, 
## it will block sending until channel is ready
storm.messaging.netty.async.block: true

# By default, the Netty SASL authentication is set to false.  Users can override and set it true for a specific topology.
storm.messaging.netty.authentication: false
# configuration for flow control in netty client
# When cache size is null, the transfer batch size of netty client will be used.
netty.client.flow.ctrl.wait.time.ms: 5
netty.client.flow.ctrl.cache.size: null

# Default plugin to use for automatic network topology discovery
storm.network.topography.plugin: backtype.storm.networktopography.DefaultRackDNSToSwitchMapping
# default number of seconds group mapping service will cache user group
storm.group.mapping.service.cache.duration.secs: 120

# netty pending buffer timeout, if pending time is longer than this setting, the pending buffer will be throw
# set it as -1, client won't send message until the channel is read
storm.messaging.netty.pending.buffer.timeout: null

### topology.* configs are for specific executing storms
topology.enable.message.timeouts: true
topology.debug: false
topology.optimize: true
topology.workers: 1
topology.acker.executors: null
topology.eventlogger.executors: null
topology.tasks: null
topology.master.single.worker: null # true/false. If null, this config is based on the num of topology workers.
# maximum amount of time a message has to complete before it's considered failed
topology.message.timeout.secs: 30
topology.multilang.serializer: "backtype.storm.multilang.JsonSerializer"
topology.shellbolt.max.pending: 100
topology.skip.missing.kryo.registrations: false
topology.max.task.parallelism: null
# topology.spout.parallelism and topology.bolt.parallelism are used 
# to change the component's parallelism when executing restart command
#topology.spout.parallelism:
#    {
#        "spoutName" : Num
#    }
#topology.bolt.parallelism: null
#    {
#        "BoltName_1" : Num,
#        "BoltName_2" : Num
#    }
topology.max.spout.pending: null
topology.state.synchronization.timeout.secs: 60
topology.stats.sample.rate: 0.05
topology.builtin.metrics.bucket.size.secs: 60
topology.fall.back.on.java.serialization: false
topology.kryo.register.required: false
topology.worker.childopts: null
topology.ctrl.buffer.size: 256 #control queue size
topology.executor.receive.buffer.size: 256 # recv buffer(deserialize and execute) of task
topology.executor.send.buffer.size: 256 # send buffer of task
topology.receiver.buffer.size: 8 # setting it too high causes a lot of problems (heartbeat thread gets starved, throughput plummets)
topology.tick.tuple.freq.secs: null
topology.worker.shared.thread.pool.size: 4
topology.disruptor.wait.timeout: 5 # ms
topology.spout.wait.strategy: "backtype.storm.spout.SleepSpoutWaitStrategy"
topology.sleep.spout.wait.strategy.time.ms: 1
topology.error.throttle.interval.secs: 10
topology.max.error.report.per.interval: 5
topology.kryo.factory: "backtype.storm.serialization.DefaultKryoFactory"
topology.tuple.serializer: "backtype.storm.serialization.types.ListDelegateSerializer"
topology.trident.batch.emit.interval.millis: 500
topology.accurate.metric: false
topology.histogram.size: 256

# jstorm metrics monitor configuration
topology.performance.metrics: true
topology.alimonitor.metrics.post: false
topology.alimonitor.topo.metrics.name: "jstorm_metric"
topology.alimonitor.task.metrics.name: "jstorm_task_metrics"
topology.alimonitor.worker.metrics.name: "jstorm_worker_metrics"
topology.alimonitor.user.metrics.name: "jstorm_user_metrics"
topology.task.error.report.interval: 60

# enable topology use user-define classloader to avoid class conflict
topology.enable.classloader: false

topology.testing.always.try.serialize: false
topology.classpath: null
topology.environment: null
topology.bolts.outgoing.overflow.buffer.enable: false
topology.max.worker.num.for.netty.metrics: 1
topology.metric.sample.rate: 0.05
topology.enable.stream.metrics: false
topology.timer.update.interval: 10
# disable some worker metrics and time realted task/comp metrics by default
topology.disabled.metric.names: "EmitTime,MsgDecodeTime"
topology.debug.metric.names: null
topology.enable.metric.debug: false

topology.backpressure.enable: true
topology.backpressure.water.mark.high: 0.8
topology.backpressure.water.mark.low: 0.2

topology.disable.loadaware: false
topology.state.checkpoint.interval.ms: 1000

# Configs for Resource Aware Scheduler
# topology priority describing the importance of the topology in decreasing importance starting from 0 (i.e. 0 is the highest priority and the priority importance decreases as the priority number increases).
# Recommended range of 0-29 but no hard limit set.
topology.priority: 29
topology.component.resources.onheap.memory.mb: 128.0
topology.component.resources.offheap.memory.mb: 0.0
topology.component.cpu.pcore.percent: 10.0
topology.worker.max.heap.size.mb: 768.0
topology.scheduler.strategy: "backtype.storm.scheduler.resource.strategies.scheduling.DefaultResourceAwareStrategy"
resource.aware.scheduler.eviction.strategy: "backtype.storm.scheduler.resource.strategies.eviction.DefaultEvictionStrategy"
resource.aware.scheduler.priority.strategy: "backtype.storm.scheduler.resource.strategies.priority.DefaultSchedulingPriorityStrategy"


# Enbale topology automatically to batch the tuples before sending them out.
# The batch size is 4.
task.batch.tuple: true
task.msg.batch.size: 20
task.msg.batch.flush.interval.millis: 2

# enable supervisor use cgroup to make resource isolation
# Before enable it, you should make sure:
#   1. Linux version (>= 2.6.18)
#   2. Have installed cgroup (check the file's existence:/proc/cgroups)
#   3. You should start your supervisor on root
# You can get more about cgroup:
#   http://t.cn/8s7nexU
#
# For cgroup root dir, the full path is ${supervisor.cgroup.basedir} + / + ${supervisor.cgroup.rootdir}.
# which should be consistent with the part configured in /etc/cgconfig.conf
supervisor.enable.cgroup: false
supervisor.cgroup.basedir: "/cgroup/cpu"
supervisor.cgroup.rootdir: "jstorm"
worker.cpu.core.upper.limit: 3

dev.zookeeper.path: "/tmp/dev-storm-zookeeper"

pacemaker.host: "localhost"
pacemaker.port: 6699
pacemaker.base.threads: 10
pacemaker.max.threads: 50
pacemaker.thread.timeout: 10
pacemaker.childopts: "-Xmx1024m"
pacemaker.auth.method: "NONE"
pacemaker.kerberos.users: []

#default storm daemon metrics reporter plugins
storm.daemon.metrics.reporter.plugins:
     - "backtype.storm.daemon.metrics.reporters.JmxPreparableReporter"


# if this configuration has been set,
# the spout or bolt will log all received tuples
# topology.debug just for logging all sent tuples 
topology.debug.recv.tuple: false

# if this configure has been set, the debug log message of send and recv tuples
# will be sampled output by the rate
topology.debug.sample.rate: 1.0d

#Usually, spout finish preparation before bolt, 
#so spout need wait several seconds so that bolt finish preparation
# the default setting is 30 seconds
spout.delay.run: 30

#Force spout use single thread
spout.single.thread: false

#When spout pending number is full, spout nextTuple will do sleep operation
spout.pending.full.sleep: true

# container setting means jstorm is running under other system, such as hadoop-yarn/Ali-Apsara
# For example, in Ali-Apsara, Fuxi start c++ container process, 
# the C++ container fork process to start nimbus or supervisor 
container.heartbeat.timeout.seconds: 240
container.heartbeat.frequence: 10


# enable java sandbox or not
java.sandbox.enable: false.

# logview port
nimbus.deamon.logview.port: 8621
supervisor.deamon.logview.port: 8622

# logview encoding
supervisor.deamon.logview.encoding: "utf-8"

# one log page size
log.page.size: 131072
# the max number of matches in a log search
max.match.per.log.search: 10
# the max number of blocks to search in a log search(every bolock size is 16K)
max.blocks.per.log.search: 1024

### when disruptor queue is full, sleep until queue isn't full
### the default disruptor  will query disruptor until queue isn't full
### this will cost much cpu
disruptor.use.sleep: true

process.launcher.enable: true
### Hown long processLauncher sleep seconds after launch one process
process.launcher.sleep.seconds: 20
process.launcher.childopts: " -Xms256m -Xmx256m  "

###supervisor health check
supervisor.enable.check: false
supervisor.frequency.check.secs: 60
storm.machine.resource.panic.check.dir: null
storm.machine.resource.error.check.dir: null
storm.machine.resource.warning.check.dir: null


###supervisor machine resource limit
storm.machine.resource.reserve.mem: 1073741824
storm.machine.resource.reserve.cpu.percent: 30

## blobstore configs

### The directory where all blobs are stored. For local file system it represents the directory on
### the nimbus node and for HDFS file system it represents the hdfs file system path.
blobstore.dir: null
blobstore.hdfs.hostname: null
blobstore.hdfs.port: null
### This configuration is meant to set the client for the supervisor in order to talk to the blob store.
### For a local file system blob store it is set to "com.alibaba.jstorm.blobstore.NimbusBlobStore"
### and for the HDFS blob store it is set to "com.alibaba.jstorm.hdfs.blobstore.HdfsClientBlobStore"
supervisor.blobstore.class: "com.alibaba.jstorm.blobstore.NimbusBlobStore"
supervisor.blobstore.download.thread.count: 5
supervisor.blobstore.download.max_retries: 3
supervisor.localizer.cache.target.size.mb: 10240
supervisor.localizer.cleanup.interval.ms: 600000
client.blobstore.class: "com.alibaba.jstorm.blobstore.NimbusBlobStore"

### Sets the blobstore implementation nimbus uses.
### For a local file system blob store it is set to "com.alibaba.jstorm.blobstore.LocalFsBlobStore"
### and for the HDFS blob store it is set to "com.alibaba.jstorm.hdfs.blobstore.HdfsBlobStore"
nimbus.blobstore.class: "com.alibaba.jstorm.blobstore.LocalFsBlobStore"
nimbus.blobstore.expiration.secs: 600

storm.blobstore.inputstream.buffer.size.bytes: 65536
### It sets the replication for each blob within the blob store. The "topology.min.replication.count" ensures
### the minimum replication the topology specific blobs are set before launching the topology. default is set to 3.
storm.blobstore.replication.factor: 3


### worker's share thread pool about flush thread
### the default min value for pool is 5 when min size is null, and the default max value is task.num * 2 when
### max size is null
worker.flush.pool.min.size: null
worker.flush.pool.max.size: null

disruptor.queue.batch.mode: true
disruptor.buffer.size: 20
disruptor.buffer.interval.millis: 5

### you can configure the task/worker's ser&deser thread num at the same time, but the task ser$deser thread num must
### are greater than zero, otherwise dead lock will happened. The default configure for task is one ser$deser thread
### , and the default for worker is 1.2*task.num deser thread. The deserialize operation tasks more time.

task.serialize.thread.num: null
task.deserialize.thread.num: null
worker.deserialize.thread.ratio: null
worker.serialize.thread.ratio: null

### Config for "snapshot state" transaction topology
transaction.exactly.once.mode: true # true: exactly once, false: at least once 
transaction.topology.reset: false
transaction.schedule.batch.delay.ms: 1000
transaction.max.pending.batch: 2
transaction.batch.snapshot.timeout: 120
transaction.cache.block.size: 134217728 # 128m
transaction.max.cache.block.num: 2
transaction.exactly.cache.type: "rocksDb"

#if set "shuffle.enable.inter.path", the tuple will firstly try to send the task in the same worker
shuffle.enable.inter.path: true
shuffle.inter.load.mark: 0.5

#jstorm.yarn.conf.blacklist: "jstorm.home,jstorm.log.dir,storm.local.dir,java.library.path"
cluster.conf.sync.enabled: true

jstorm.reserve.workers: 0
